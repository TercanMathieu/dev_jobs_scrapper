# Configuration Claude Code pour Dev Jobs Scrapper

Ce fichier donne le contexte Ã  Claude Code quand tu travailles sur ce projet.

## ğŸ¯ Projet

**Nom:** Dev Jobs Scrapper  
**Type:** Scraper de jobs tech avec dashboard web  
**Stack:** Python, Flask, MongoDB, Docker, Selenium

## ğŸ“ Structure

```
â”œâ”€â”€ srcs/                      # Code source du scrapper
â”‚   â”œâ”€â”€ common/               # Modules partagÃ©s
â”‚   â”‚   â”œâ”€â”€ database.py       # Connexion MongoDB
â”‚   â”‚   â”œâ”€â”€ discord_logger.py # Logging Discord
â”‚   â”‚   â”œâ”€â”€ job_analyzer.py   # Analyse des fiches de poste
â”‚   â”‚   â”œâ”€â”€ webhook.py        # Envoi Discord
â”‚   â”‚   â””â”€â”€ website.py        # Classe base pour les scrapers
â”‚   â”œâ”€â”€ websites/             # Scrapers spÃ©cifiques
â”‚   â”‚   â”œâ”€â”€ wttj.py          # Welcome to the Jungle
â”‚   â”‚   â”œâ”€â”€ jobteaser.py     # Job Teaser
â”‚   â”‚   â””â”€â”€ stationf.py      # Station F
â”‚   â””â”€â”€ main.py              # Point d'entrÃ©e
â”œâ”€â”€ dashboard/               # Interface web (Flask)
â”‚   â”œâ”€â”€ app.py              # API REST
â”‚   â”œâ”€â”€ templates/          # HTML
â”‚   â””â”€â”€ static/             # CSS, JS
â”œâ”€â”€ scripts/                # Scripts utilitaires
â”‚   â”œâ”€â”€ migrate_remote_days.py
â”‚   â””â”€â”€ fix_database.py
â”œâ”€â”€ docker-compose.yml      # Orchestration
â””â”€â”€ .env                    # Variables d'environnement
```

## ğŸ³ Docker

```bash
# DÃ©marrer tout
docker-compose up -d

# Voir les logs
docker-compose logs -f scrapper
docker-compose logs -f dashboard

# Rebuild
docker-compose up -d --build

# ExÃ©cuter un script
docker compose exec scrapper python scripts/migrate_remote_days.py
```

## ğŸ”Œ API Endpoints (Dashboard)

- `GET /` - Dashboard
- `GET /jobs` - Liste des jobs avec filtres
- `GET /analytics` - Graphiques et stats
- `GET /api/jobs` - API jobs (JSON)
- `GET /api/analytics/*` - Stats pour les graphiques

## ğŸ’¾ MongoDB

**Database:** `jobs_database`  
**Collections:**
- `jobs_collection` - Les offres d'emploi
- `logs` - Logs du systÃ¨me

## ğŸ“ Conventions de code

- Python 3.11+
- Type hints quand pertinent
- Docstrings en franÃ§ais
- Variables en snake_case

## ğŸš¨ Points d'attention

1. **Scrapers:** Peuvent casser si les sites changent leur HTML
2. **Selenium:** NÃ©cessite Chrome/Chromium dans le conteneur
3. **Rate limiting:** DÃ©lai de 4s entre chaque job pour Ã©viter le blocage
4. **MongoDB:** Connexion via `mongodb://mongodb:27017/`

## ğŸ› ï¸ TÃ¢ches communes

### Ajouter un nouveau site de scraping
1. CrÃ©er `srcs/websites/nomdujob.py`
2. HÃ©riter de `Website` dans `common.website`
3. ImplÃ©menter la mÃ©thode `scrap()`
4. Ajouter Ã  `main.py`

### Modifier le dashboard
1. Ã‰diter `dashboard/app.py` pour l'API
2. Ã‰diter `dashboard/templates/` pour l'HTML
3. Ã‰diter `dashboard/static/` pour CSS/JS
4. Rebuild: `docker-compose up -d --build dashboard`

### Migrer des donnÃ©es
```bash
docker compose exec scrapper python scripts/NOM_DU_SCRIPT.py
```

## ğŸ”— Liens utiles

- Dashboard: http://localhost:8080
- Dashboard jobs: http://localhost:8080/jobs
- Dashboard analytics: http://localhost:8080/analytics
- MongoDB: localhost:27017

## ğŸ“¦ DÃ©pendances principales

**Scrapper:**
- selenium (web scraping)
- beautifulsoup4 (parsing HTML)
- pymongo (base de donnÃ©es)
- discord-webhook (notifications)
- requests (HTTP)

**Dashboard:**
- flask (serveur web)
- pymongo
- chart.js (graphiques, via CDN)
